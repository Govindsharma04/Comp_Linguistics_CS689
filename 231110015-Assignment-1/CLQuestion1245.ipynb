{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Defining a function which will take a word and will correct its unicode representation\n",
        "def unicode_correction(word):\n",
        "    unicode_word = []\n",
        "    length = len(word)\n",
        "    for i in range(length):\n",
        "        if word[i] in punctuation_characters:\n",
        "            continue\n",
        "        if word[i]=='्':\n",
        "            continue\n",
        "        if word[i] in List_swaras:\n",
        "            unicode_word.append(word[i])\n",
        "        elif word[i] in List_matras:\n",
        "            unicode_word.append(mapping_dict[word[i]])\n",
        "        elif i!=length-1 and word[i+1] in List_matras:\n",
        "            unicode_word.append(word[i]+'्')\n",
        "        elif i!=length-1 and word[i+1] in List_swaras:\n",
        "            unicode_word.append(word[i]+'्')\n",
        "        else:\n",
        "            if i != length-1 and word[i+1] == '्':\n",
        "                unicode_word.append(word[i]+'्')\n",
        "                continue;\n",
        "            unicode_word.append(word[i]+'्')\n",
        "            unicode_word.append('अ')\n",
        "    return unicode_word\n",
        "\n",
        "# Defining a function which will remove unnecessory punctuation signs\n",
        "def clean(token_list):\n",
        "  newlist=[]\n",
        "  for tokens in token_list:\n",
        "    str=\"\"\n",
        "    for ch in tokens:\n",
        "      if ch not in punctuation_characters:\n",
        "        str+=ch\n",
        "    newlist.append(str)\n",
        "  return newlist\n",
        "# defining a function to break text into list of words using regular expression\n",
        "def extract_words(text):\n",
        "    pattern = r'[\\u0900-\\u097F]+'\n",
        "    words = re.findall(pattern, text)\n",
        "    return words\n",
        "\n",
        "# defining function to break text into list of characters\n",
        "def break_text(text):\n",
        "    words = extract_words(text)\n",
        "    broken_text = []\n",
        "    for word in words:\n",
        "        broken_word = unicode_correction(word)\n",
        "        broken_text.append((word, broken_word))\n",
        "    return broken_text\n",
        "\n",
        "# defining function to load corpus from file\n",
        "def load_corpus(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        # Reading only the first 1 lines which we can change anytimme\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Creating bigrams from a given Unicode word and returning list of bigram extracted from it\n",
        "def create_bigrams(unicode_word):\n",
        "\n",
        "    bigram_word = [unicode_word[i] + unicode_word[i+1] for i in range(len(unicode_word)-1)]\n",
        "    return bigram_word\n",
        "\n",
        "def create_syllable_bigrams(unicode_word):\n",
        "    # Generate syllable bigrams\n",
        "    syllable_bigram_word = [unicode_word[i] + unicode_word[i+1] for syllable in unicode_word for i in range(len(syllable)-1)]\n",
        "\n",
        "    return syllable_bigram_word\n",
        "\n",
        "# defining function to calculate bigram frequencies\n",
        "def calculate_bigram_frequencies(text):\n",
        "    bigram_freq = {}\n",
        "    for _, unicode_word in text:\n",
        "        for i in range(len(unicode_word) - 1):\n",
        "            bigram = unicode_word[i] + unicode_word[i+1]\n",
        "            if bigram in bigram_freq:\n",
        "                bigram_freq[bigram] += 1\n",
        "            else:\n",
        "                bigram_freq[bigram] = 1\n",
        "    sorted_bigram_freq = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_bigram_freq\n",
        "\n",
        "def form_syllable(unicode_word_list):\n",
        "    syllable_list = []\n",
        "    for word in unicode_word_list:\n",
        "        syllable = \"\"\n",
        "        temp = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            char = word[i]\n",
        "            if char in List_vyanjan_halant:\n",
        "                if char == '्' and syllable.endswith('|'):  # for specific case of '|'\n",
        "                    syllable += char\n",
        "                elif i+1 < len(word) and word[i+1] in List_matras:\n",
        "                    syllable += char + word[i+1]\n",
        "                    i += 1  # Skip the next character (matra) as it's already included\n",
        "                else:\n",
        "                    if syllable:\n",
        "                        temp.append(syllable)\n",
        "                    syllable = char\n",
        "            else:\n",
        "                syllable += char\n",
        "            i += 1\n",
        "\n",
        "        if syllable:  # Append the remaining syllable\n",
        "            temp.append(syllable)\n",
        "        if temp:\n",
        "            syllable_list.append(temp)\n",
        "    return syllable_list\n",
        "\n",
        "# defining function to calculate characters frequencies\n",
        "def calculate_character_frequencies(broken_text):\n",
        "    character_frequency = {}\n",
        "    for _, unicode_word in broken_text:\n",
        "        for char in unicode_word:\n",
        "            if char in character_frequency:\n",
        "                character_frequency[char] += 1\n",
        "            else:\n",
        "                character_frequency[char] = 1\n",
        "    sorted_characters = sorted(character_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_characters\n",
        "\n",
        "# defining function to calculate syllable bigram frequencies\n",
        "def calculate_syllable_frequencies(syllable_list):\n",
        "    syllable_freq = {}\n",
        "    for syllable in syllable_list:\n",
        "        for sub_syllable in syllable:\n",
        "            if sub_syllable in syllable_freq:\n",
        "                syllable_freq[sub_syllable] += 1\n",
        "            else:\n",
        "                syllable_freq[sub_syllable] = 1\n",
        "    sorted_syllables = sorted(syllable_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_syllables"
      ],
      "metadata": {
        "id": "otTmGo1G70mF"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QN4xRFKaKec"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Defining the Unicode mapping dictionary\n",
        "# List of hindi matras and swaras and vyanjan\n",
        "List_matras = ['ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', 'ँ']\n",
        "List_swaras = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ','अँ']\n",
        "#string of punctuation_characters we need to avoid\n",
        "punctuation_characters = \"##▁#।▁,?!.:;‘’“”-…()▁\"\n",
        "Alphabet_eng =['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "List_vyanjan = ['क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', 'क्ष', 'त्र', 'ज्ञ']\n",
        "List_vyanjan_halant = [char + '्' for char in List_vyanjan]\n",
        "# dictionary of mapping of matras to swaras\n",
        "mapping_dict = {\"ा\": \"आ\", \"ि\": \"इ\", \"ी\": \"ई\", \"ु\": \"उ\", \"ू\": \"ऊ\", \"े\": \"ए\", \"ै\": \"ऐ\", \"ो\": \"ओ\", \"ौ\": \"औ\", \"ं\": \"अं\", \"ृ\": \"ऋ\", \"ॉ\": \"ऑ\", \"्\": \"\", \"ः\": \"ः\", 'ँ': \"अँ\"}\n",
        "# dictionary of swaras to matras aka reverse of mapping_dict\n",
        "reverse_mapping_dict = {'आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ', 'अं': 'ं', 'ऋ': 'ृ', 'ऑ': 'ॉ','ॅ्':'', 'ः': 'ः','\\n्':'', 'अँ': 'ँ','ळ्':'','अ': '', '़्': '','ृ्':'','ॉ्':''}\n",
        "Alphabet_eng =['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "# Loading text from corpus file\n",
        "corpus_file_path = \"sample.txt\"\n",
        "text = load_corpus(corpus_file_path)\n",
        "\n",
        "# Breaking the text into list of characters\n",
        "broken_text = break_text(text)\n",
        "\n",
        "# Printing the broken text along with the original words\n",
        "for word, unicode_word in broken_text:\n",
        "    print(f\"Extracted Word :- {word} :-\\n{unicode_word}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Defining the Unicode mapping dictionary\n",
        "# List of hindi matras and swaras and vyanjan\n",
        "\n",
        "# Loading text from corpus file\n",
        "corpus_file_path = \"sample.txt\"\n",
        "text = load_corpus(corpus_file_path)\n",
        "\n",
        "# Breaking the text into list of characters\n",
        "broken_text = break_text(text)\n",
        "print(\"bt\",broken_text)\n",
        "# Initialize lists to store Unicode-corrected words\n",
        "unicode_word_list = []\n",
        "\n",
        "# Printing the broken text along with the original words, syllables, and bigrams\n",
        "for word, unicode_word in broken_text:\n",
        "    unicode_word_list.append(unicode_word)\n",
        "    bigram_word = create_bigrams(unicode_word)\n",
        "    syllable_list = form_syllable([unicode_word])  # Call form_syllable for the current word\n",
        "    syllable_list_bigrams = create_bigrams(syllable_list)\n",
        "\n",
        "    # print(f\"Extracted Word: {word}:\")\n",
        "    # print(f\"Unicode-corrected Characters: {unicode_word}\")\n",
        "    # print(f\"Unicode Bigram: {bigram_word}\")\n",
        "    # print(f\"Syllables: {syllable_list}\")\n",
        "    # print(f\"Syllables Bigram: {syllable_list_bigrams}\\n\")\n",
        "\n",
        "print(\"uwl\",unicode_word_list)\n",
        "# Calling the form_syllable function\n",
        "syllable_list = form_syllable(unicode_word_list)\n",
        "\n",
        "# Printing the syllables in a similar format as characters and bigrams\n",
        "syllable_bigram_list = [syllable_list[i]+syllable_list[i+1] for i in range(len(syllable_list)-1)]\n",
        "# Calculating the frequency of each character\n",
        "\n",
        "# Sorting the characters by frequency in descending order as we need the most freq char on top\n",
        "sorted_characters = calculate_character_frequencies(broken_text)\n",
        "# Calculating bigram frequencies\n",
        "bigram_freq = calculate_bigram_frequencies(broken_text)\n",
        "\n",
        "# Printing the top 20 unicode corrected characters and their frequencies\n",
        "print(\"Top 20 characters:\")\n",
        "for char, frequency in sorted_characters[:20]:\n",
        "    print(f\"Character: {char}, Frequency: {frequency}\")\n",
        "\n",
        "# Print the top 20 bigram and their frequencies\n",
        "print(\"\\nTop 20 bigram frequencies:\")\n",
        "for bigram, frequency in bigram_freq[:20]:\n",
        "    print(f\"Bigram: {''.join(bigram)}, Frequency: {frequency}\")\n",
        "\n",
        "# Calculating the syllable frequencies\n",
        "\n",
        "# Sorting the syllables by frequency in descending order\n",
        "sorted_syllables = calculate_syllable_frequencies(syllable_list)\n",
        "\n",
        "# Printing the top 20 syllable frequencies\n",
        "print(\"\\nTop 20 syllable frequencies:\")\n",
        "for syllable, frequency in sorted_syllables[:20]:\n",
        "    print(f\"Syllable: {''.join(syllable)}, Frequency: {frequency}\")\n",
        "\n",
        "# print(\"\\nTop 20 syllable bigram frequencies:\")\n",
        "# for syllable, frequency in sorted_syllables[:20]:\n",
        "#     print(f\"Syllable: {''.join(syllable)}, Frequency: {frequency}\")\n"
      ],
      "metadata": {
        "id": "3DALIoihtjF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0631ec0c-7afb-4cde-f5d3-cdaa5a40a9b0"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bt [('आवेदन', ['आ', 'व्', 'ए', 'द्', 'अ', 'न्', 'अ']), ('करने', ['क्', 'अ', 'र्', 'अ', 'न्', 'ए']), ('की', ['क्', 'ई']), ('आखिरी', ['आ', 'ख्', 'इ', 'र्', 'ई']), ('तारीख', ['त्', 'आ', 'र्', 'ई', 'ख्', 'अ']), ('जनवरी', ['ज्', 'अ', 'न्', 'अ', 'व्', 'अ', 'र्', 'ई']), ('है।', ['ह्', 'ऐ'])]\n",
            "uwl [['आ', 'व्', 'ए', 'द्', 'अ', 'न्', 'अ'], ['क्', 'अ', 'र्', 'अ', 'न्', 'ए'], ['क्', 'ई'], ['आ', 'ख्', 'इ', 'र्', 'ई'], ['त्', 'आ', 'र्', 'ई', 'ख्', 'अ'], ['ज्', 'अ', 'न्', 'अ', 'व्', 'अ', 'र्', 'ई'], ['ह्', 'ऐ']]\n",
            "Top 20 characters:\n",
            "Character: अ, Frequency: 8\n",
            "Character: र्, Frequency: 4\n",
            "Character: ई, Frequency: 4\n",
            "Character: आ, Frequency: 3\n",
            "Character: न्, Frequency: 3\n",
            "Character: व्, Frequency: 2\n",
            "Character: ए, Frequency: 2\n",
            "Character: क्, Frequency: 2\n",
            "Character: ख्, Frequency: 2\n",
            "Character: द्, Frequency: 1\n",
            "Character: इ, Frequency: 1\n",
            "Character: त्, Frequency: 1\n",
            "Character: ज्, Frequency: 1\n",
            "Character: ह्, Frequency: 1\n",
            "Character: ऐ, Frequency: 1\n",
            "\n",
            "Top 20 bigram frequencies:\n",
            "Bigram: अन्, Frequency: 3\n",
            "Bigram: र्ई, Frequency: 3\n",
            "Bigram: न्अ, Frequency: 2\n",
            "Bigram: अर्, Frequency: 2\n",
            "Bigram: आव्, Frequency: 1\n",
            "Bigram: व्ए, Frequency: 1\n",
            "Bigram: एद्, Frequency: 1\n",
            "Bigram: द्अ, Frequency: 1\n",
            "Bigram: क्अ, Frequency: 1\n",
            "Bigram: र्अ, Frequency: 1\n",
            "Bigram: न्ए, Frequency: 1\n",
            "Bigram: क्ई, Frequency: 1\n",
            "Bigram: आख्, Frequency: 1\n",
            "Bigram: ख्इ, Frequency: 1\n",
            "Bigram: इर्, Frequency: 1\n",
            "Bigram: त्आ, Frequency: 1\n",
            "Bigram: आर्, Frequency: 1\n",
            "Bigram: ईख्, Frequency: 1\n",
            "Bigram: ख्अ, Frequency: 1\n",
            "Bigram: ज्अ, Frequency: 1\n",
            "\n",
            "Top 20 syllable frequencies:\n",
            "Syllable: र्ई, Frequency: 3\n",
            "Syllable: आ, Frequency: 2\n",
            "Syllable: न्अ, Frequency: 2\n",
            "Syllable: व्ए, Frequency: 1\n",
            "Syllable: द्अ, Frequency: 1\n",
            "Syllable: क्अ, Frequency: 1\n",
            "Syllable: र्अ, Frequency: 1\n",
            "Syllable: न्ए, Frequency: 1\n",
            "Syllable: क्ई, Frequency: 1\n",
            "Syllable: ख्इ, Frequency: 1\n",
            "Syllable: त्आ, Frequency: 1\n",
            "Syllable: ख्अ, Frequency: 1\n",
            "Syllable: ज्अ, Frequency: 1\n",
            "Syllable: व्अ, Frequency: 1\n",
            "Syllable: ह्ऐ, Frequency: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sentencepiece as spm\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "# Load the corpus from file\n",
        "\n",
        "corpus_file = \"hi_100.txt\"\n",
        "\n",
        "# Train Unigram model using SentencePiece\n",
        "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix='unigram_model', model_type='unigram', vocab_size = 1000)\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('unigram_model.model')\n",
        "print(\"Unigram model trained successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgD-94JFDFOP",
        "outputId": "a7b50eab-763e-45e1-8a27-a2b88dc9084a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram model trained successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Training our model using SentencePiece for vocab_size=1000\n",
        "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix='bpe_model_1000', model_type='bpe', vocab_size=1000)\n",
        "\n",
        "print(\"BPE model trained successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhzYNXif3cy0",
        "outputId": "08764845-6b45-4bf6-be33-8b4fed9e7305"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE model trained successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training our model using SentencePiece for vocab_size=2000\n",
        "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix='bpe_model_2000', model_type='bpe', vocab_size=2000)\n",
        "\n",
        "print(\"BPE model trained successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjxwcZPDStnK",
        "outputId": "c49b13af-bf67-4d5d-a77e-c8ae277d870c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE model trained successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer\n",
        "\n",
        "tokenizer_mbert1 = BertTokenizer.from_pretrained('bert-base-multilingual-cased' , max_length=1000, truncation=True)\n"
      ],
      "metadata": {
        "id": "x63HHZbz4vpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_mbert1 = BertTokenizer.from_pretrained('bert-base-multilingual-cased' , max_length=2000, truncation=True)\n"
      ],
      "metadata": {
        "id": "oP1RF4rgO31g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n"
      ],
      "metadata": {
        "id": "Boh_b25MdYpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cbslist(unigram_tokens):\n",
        "  # Remove leading '▁' from tokens and create unigrams\n",
        "  unigram_op1 = [tokens.replace('▁', '') if tokens.startswith('▁') else tokens for tokens in unigram_tokens]\n",
        "  unigram_op2 = [i for i in unigram_op1 if len(i) > 0 and (i[0] in List_matras or i[0] in List_swaras or i[0] in List_vyanjan)]\n",
        "  broken_text = [(i,unicode_correction(i)) for i in unigram_op2]\n",
        "\n",
        "  unicode_word_list = []\n",
        "\n",
        "  # Printing the broken text along with the original words, syllables, and bigrams\n",
        "  for word, unicode_word in broken_text:\n",
        "      unicode_word_list.append(unicode_word)\n",
        "      bigram_word = create_bigrams(unicode_word)\n",
        "      syllable_list = form_syllable([unicode_word])  # Call form_syllable for the current word\n",
        "      syllable_list_bigrams = create_bigrams(syllable_list)\n",
        "\n",
        "      # print(f\"Extracted Word: {word}:\")\n",
        "      # print(f\"Unicode-corrected Characters: {unicode_word}\")\n",
        "      # print(f\"Unicode Bigram: {bigram_word}\")\n",
        "      # print(f\"Syllables: {syllable_list}\")\n",
        "      # print(f\"Syllables Bigram: {syllable_list_bigrams}\\n\")\n",
        "\n",
        "  print(\"uwl\",unicode_word_list)\n",
        "  # Calling the form_syllable function\n",
        "  syllable_list = form_syllable(unicode_word_list)\n",
        "\n",
        "  # Printing the syllables in a similar format as characters and bigrams\n",
        "  syllable_bigram_list = [syllable_list[i]+syllable_list[i+1] for i in range(len(syllable_list)-1)]\n",
        "\n",
        "  # Calculating the frequency of each character\n",
        "\n",
        "  # Sorting the characters by frequency in descending order as we need the most freq char on top\n",
        "  sorted_characters = calculate_character_frequencies(broken_text)\n",
        "  # Calculating bigram frequencies\n",
        "  bigram_freq = calculate_bigram_frequencies(broken_text)\n",
        "\n",
        "  # Printing the top 20 unicode corrected characters and their frequencies\n",
        "  print(\"Top 20 characters:\")\n",
        "  for char, frequency in sorted_characters[:20]:\n",
        "      print(f\"Character: {char}, Frequency: {frequency}\")\n",
        "\n",
        "  # Print the top 20 bigram and their frequencies\n",
        "  print(\"\\nTop 20 bigram frequencies:\")\n",
        "  for bigram, frequency in bigram_freq[:20]:\n",
        "      print(f\"Bigram: {''.join(bigram)}, Frequency: {frequency}\")\n",
        "\n",
        "  # Calculating the syllable frequencies\n",
        "\n",
        "  # Sorting the syllables by frequency in descending order\n",
        "  sorted_syllables = calculate_syllable_frequencies(syllable_list)\n",
        "\n",
        "  # Printing the top 20 syllable frequencies\n",
        "  print(\"\\nTop 20 syllable frequencies:\")\n",
        "  for syllable, frequency in sorted_syllables[:20]:\n",
        "      print(f\"Syllable: {''.join(syllable)}, Frequency: {frequency}\")\n"
      ],
      "metadata": {
        "id": "4zST_N4GeiDh"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'sp_unigram' is imported and defined properly\n",
        "from transformers import BertTokenizer, AutoTokenizer\n",
        "# Tokenize text\n",
        "with open('sample.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "unigram_tokens = sp_unigram.encode_as_pieces(text)\n",
        "\n",
        "cbslist(unigram_tokens)"
      ],
      "metadata": {
        "id": "D9RZgjq2TJIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the BPE tokenizer\n",
        "tokenizer_bpe = BertTokenizer.from_pretrained(\"bert-base-uncased\",max_length=1000)\n",
        "\n",
        "# Tokenize text using BPE model\n",
        "bpe_tokens = tokenizer_bpe.tokenize(text)\n",
        "\n",
        "cbslist(bpe_tokens)"
      ],
      "metadata": {
        "id": "3VCf27DKbvtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer_bpe_2000 = BertTokenizer.from_pretrained(\"bert-base-uncased\",max_length=2000)\n",
        "bpe_tokens = tokenizer_bpe_2000.tokenize(text)\n",
        "\n",
        "cbslist(bpe_tokens)"
      ],
      "metadata": {
        "id": "cE6YHbHBTsRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\",max_length=1000)\n",
        "\n",
        "# Tokenize text\n",
        "with open('sample.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text\n",
        "mbert_tokens = tokenizer_bert.tokenize(text)\n",
        "\n",
        "cbslist(mbert_tokens)"
      ],
      "metadata": {
        "id": "mHLoPYfFZaXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer_bert_2000 = BertTokenizer.from_pretrained('bert-base-multilingual-cased',max_length=2000)\n",
        "\n",
        "# Tokenize text\n",
        "with open('sample.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text\n",
        "mbert_tokens = tokenizer_bert_2000.tokenize(text)\n",
        "\n",
        "cbslist(mbert_tokens)"
      ],
      "metadata": {
        "id": "DrN4jYhCbL9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_indic = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "\n",
        "indicbert_tokens = tokenizer_indic.tokenize(tokenizer.decode(tokenizer.encode(text, max_length=1000, truncation=True)))\n",
        "\n",
        "cbslist(indicbert_tokens)"
      ],
      "metadata": {
        "id": "yc586xDCghmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_indic_2000 = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "\n",
        "indicbert_tokens = tokenizer_indic_2000.tokenize(tokenizer.decode(tokenizer.encode(text, max_length=2000, truncation=True)))\n",
        "\n",
        "cbslist(indicbert_tokens)"
      ],
      "metadata": {
        "id": "bg9xxyaRhkVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wst_tokens = text.split(\" \")\n",
        "cbslist(wst_tokens)"
      ],
      "metadata": {
        "id": "ofq94lr8izk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = [\n",
        "    ['ढाबों में','रेट लिस्ट', 'जरूर'],\n",
        "    ['ये गाना', 'पब और लाउंज के लिए', 'फिट बैठता है', 'इसलिए', 'मेकर्स ने', 'इसे पब में रिलीज करने का फैसला किया'],\n",
        "    ['आप', 'यह पावरप्वॉइण्ट फाइल', 'कोकाकोला के चित्र', 'पर', 'क्लिक कर', 'डाउनलोड कर', 'सकते हैं'],\n",
        "    ['मार्सिलिया (Marsilea) तथा सिरेटोप्टेरिस (Ceratopteris)', 'जैसे', 'टेरिडोफाइट्स का', 'उपयोग', 'सब्जी के रूप में', 'होता है'],\n",
        "    ['बैठक के बाद', 'से', 'मीडिया में', 'कयास लगाए जाने लगे थे', 'कि', 'जम्मू-कश्मीर पर', 'केंद्र सरकार ने', 'कोई बड़ा फैसला', 'ले लिया है'],\n",
        "    ['उन्होंने कहा', 'डॉ राजेन्द्र प्रसाद', 'बाबा साहेब भीमराव अंबेडकर', 'सरदार पटेल', 'मौलाना आजाद', 'सुचेता कृपलानी', 'और', 'अनेक अनगिनत महापुरुषों', 'ने', 'प्रत्यक्ष और अप्रत्यक्ष', 'योगदान', 'देकर', 'ये महान विरासत', 'हमें', 'सौंपी हैं'],\n",
        "    ['अकसर', 'हम', 'ऐसे', 'सवालों से', 'जूझना', 'ज़रूरी', 'नहीं', 'समझते'],\n",
        "    ['यह', 'उपयोगी था।', 'इस सुविधा', 'को', \"'निष्क्रिय करने'\", 'के बाद', 'चीजें', 'बहुत', 'स्पष्ट थीं', 'और', 'मैं', 'खाली पैकेजों', 'को', 'हटा सकता थ'],\n",
        "    ['करण जौहर', 'ने', 'यह तस्वीर अपने', 'इंस्टाग्राम पर', 'शेयर', 'की है', 'जिसमें', 'आलिया भट्ट', 'उनके', 'बेटे', 'यश', 'को', 'राखी बांधती', 'नजर आ रही हैं'],\n",
        "    ['जयपुर :', 'राजस्थान में', 'गहलोत मंत्रिमंडल', 'गठन में', 'लोकसभा चुनाव', 'की', 'छाया', 'रहेगी'],\n",
        "    ['उन्होंने', 'मुख्य रूप से', 'प्रदेश की', 'सभी मार्गों को', 'गड्ढा मुक्त करने हेतु', 'आवश्यक मरम्मत कार्य', 'गुणवत्ता के साथ', 'शीघ्र पूर्ण करने', 'कहा'],\n",
        "    ['थोक बाजार में', 'टमाटर', 'एक दिन में', '40 फीसदी तक', 'महंगा', 'हो रहा है'],\n",
        "    ['उनके मुताबिक़', \"''ऑक्सीजन की बात\", 'किसी को', 'बताई नहीं गई थी', 'लेकिन', 'मेरी बच्ची के मुंह से', 'जब ख़ून निकलने लगा', 'तो', 'समझ में आया', 'कि', 'वो दम घुटने से मरी है'],\n",
        "    ['नई दिल्ली:', 'टीम इंडिया के', 'उभरते बल्लेबाजी-ऑलराउंडर', 'हार्दिक पंड्या के', 'लिए दक्षिण अफ्रीकी दौरा', 'उनकी और करोड़ों भारतीय क्रिकेटप्रेमियों के लिए', 'निराशाजनक', 'साबित हुआ'],\n",
        "    ['सवाल :', 'गोलकीपर सविता (पुनिया)', 'ने', 'टूर्नामेंट में', 'शानदार प्रदर्शन', 'किया'],\n",
        "    ['इस संदर्भ में', 'कोई', 'पक्का साक्ष्य', 'नहीं', 'मिला है'],\n",
        "    ['सूत्र बताते हैं', 'कि', 'अपनी बातचीत', 'को', 'अपलोड करने', 'का', 'कारण', 'यह', 'था', 'कि', 'इस तरह', 'कापड़ी', 'की', 'मक्खन-मालिश', 'हो जाएगी'],\n",
        "    ['उन्होेंने बताया', 'कि', 'इन योजनाओं', 'के बन जाने से', 'जहां', '1250 हेक्टेयर', 'भूमि', 'सिंचित', 'होगी'],\n",
        "    ['उसने तुरंत', 'बैंक में जाकर', 'पता किया', 'तो', 'उसके खाते से', 'किसी ने', 'एटीएम के माध्यम से', 'तीस हजार रुपये', 'निकाल लिए थे'],\n",
        "    ['बाल्को', 'पॉट लाइन', 'प्रचालन प्रमुख', 'निकेत श्रीवास्तव', 'प्रभारी रूम-टू', 'पवन एम पाटिल', 'और', 'पॉट लाइन अनुरक्षण प्रमुख', 'दुर्गा प्रसाद पांडा', 'ने', 'बाल्कोकर्मियों', 'और', 'ठेका कामगारों', 'को', 'सम्मानित', 'किय'],\n",
        "    ['क्या यह', 'प्रैक्टिकल है', 'या', 'सिर्फ', 'एक कहानी'],\n",
        "    ['एनडीए में', 'बिहार के', 'मुख्यमंत्री', 'नीतीश कुमार', 'की', 'पार्टी', 'जनता दल यूनाइटेड', '(जेडीयू)', 'भी', 'शामिल', 'है'],\n",
        "    ['नवविवाहित', 'प्रेमी जोड़ों को', 'सुरक्षित आश्रय स्थल', 'प्रदान करने के लिए', 'जेल परिसर में', 'सुरक्षित आश्रय', 'भवन की', 'व्यवस्था', 'की', 'गई है'],\n",
        "    ['एएसआई जगदीश ने', 'बताया कि', 'युवक की जेब की तलाशी लेने पर', 'एक मोबाइल के अलावा', 'कुछ नहीं मिला है'],\n",
        "    ['गुरुग्राम में', 'साइकिल रैली में', 'हिस्सा लेते', 'अशोक तंवर']\n",
        "]\n",
        "pre = [\"ढाबों में रेट लिस्ट जरूर\", \"ये गाना पब और लाउंज के लिए फिट बैठता है इसलिए मेकर्स ने इसे पब में रिलीज करने का फैसला किया\", \"आप यह पावरप्वॉइण्ट फाइल कोकाकोला के चित्र पर क्लिक कर डाउनलोड कर सकते हैं\", \"मार्सिलिया (Marsilea) तथा सिरेटोप्टेरिस (Ceratopteris) जैसे टेरिडोफाइट्स का उपयोग सब्जी के रूप में होता है\", \"बैठक के बाद से मीडिया में कयास लगाए जाने लगे थे कि जम्मू-कश्मीर पर केंद्र सरकार ने कोई बड़ा फैसला ले लिया है\", \"उन्होंने कहा, डॉ राजेन्द्र प्रसाद, बाबा साहेब भीमराव अंबेडकर, सरदार पटेल, मौलाना आजाद, सुचेता कृपलानी और अनेक अनगिनत महापुरुषों ने प्रत्यक्ष और अप्रत्यक्ष योगदान देकर ये महान विरासत हमें सौंपी हैं\", \"अकसर हम ऐसे सवालों से जूझना ज़रूरी नहीं समझते\", \"यह उपयोगी था। इस सुविधा को 'निष्क्रिय करने' के बाद चीजें बहुत स्पष्ट थीं और मैं खाली पैकेजों को हटा सकता थ\", \"करण जौहर ने यह तस्वीर अपने इंस्टाग्राम पर शेयर की है, जिसमें आलिया भट्ट उनके बेटे यश को राखी बांधती नजर आ रही हैं\", \"जयपुर : राजस्थान में गहलोत मंत्रिमंडल गठन में लोकसभा चुनाव की छाया रहेगी\", \"उन्होंने मुख्य रूप से प्रदेश की सभी मार्गों को गड्ढा मुक्त करने हेतु आवश्यक मरम्मत कार्य गुणवत्ता के साथ शीघ्र पूर्ण करने कहा\", \"थोक बाजार में टमाटर एक दिन में 40 फीसदी तक महंगा हो रहा है\", \"उनके मुताबिक़, ''ऑक्सीजन की बात किसी को बताई नहीं गई थी, लेकिन मेरी बच्ची के मुंह से जब ख़ून निकलने लगा तो समझ में आया कि वो दम घुटने से मरी है\", 'नई दिल्ली: टीम इंडिया के उभरते बल्लेबाजी-ऑलराउंडर हार्दिक पंड्या के लिए दक्षिण अफ्रीकी दौरा उनकी और करोड़ों भारतीय क्रिकेटप्रेमियों के लिए निराशाजनक साबित हुआ.', 'सवाल : गोलकीपर सविता (पुनिया) ने टूर्नामेंट में शानदार प्रदर्शन किया', 'इस संदर्भ में कोई पक्का साक्ष्य नहीं मिला है', 'सूत्र बताते हैं कि अपनी बातचीत को अपलोड करने का कारण यह था कि इस तरह कापड़ी की मक्खन-मालिश हो जाएगी।', 'उन्होेंने बताया कि इन योजनाओं के बन जाने से जहां 1250 हेक्टेयर भूमि सिंचित होगी', \"उसने तुरंत बैंक में जाकर पता किया तो उसके खाते से किसी ने एटीएम के माध्यम से तीस हजार रुपये निकाल लिए थे\", \"बाल्को के पॉट लाइन प्रचालन प्रमुख निकेत श्रीवास्तव, प्रभारी रूम-टू पवन एम पाटिल और पॉट लाइन अनुरक्षण प्रमुख दुर्गा प्रसाद पांडा ने बाल्कोकर्मियों और ठेका कामगारों को सम्मानित किया\", \"क्या यह प्रैक्टिकल है या सिर्फ एक कहानी\", \"एनडीए में बिहार के मुख्यमंत्री नीतीश कुमार की पार्टी जनता दल यूनाइटेड (जेडीयू) भी शामिल है\", \"नवविवाहित प्रेमी जोड़ों को सुरक्षित आश्रय स्थल प्रदान करने के लिए जेल परिसर में सुरक्षित आश्रय भवन की व्यवस्था की गई है\", \"एएसआई जगदीश ने बताया कि युवक की जेब की तलाशी लेने पर एक मोबाइल के अलावा कुछ नहीं मिला है\", \"गुरुग्राम में साइकिल रैली में हिस्सा लेते अशोक तंवर\"]"
      ],
      "metadata": {
        "id": "ztZJcIbrpDXp"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc(ans, unigram_t):\n",
        "    tp = fp = fn = 0\n",
        "\n",
        "    for true_tokens, pred_tokens in zip(ans, unigram_t):\n",
        "        true_set = set(true_tokens)\n",
        "        pred_set = set(clean(pred_tokens))\n",
        "\n",
        "        tp += len(true_set.intersection(pred_set))\n",
        "        fp += len(pred_set - true_set)\n",
        "        fn += len(true_set - pred_set)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f_score\n",
        "\n",
        "unigram_t=[]\n",
        "for line in pre:\n",
        "  unigram_t.append(sp_unigram.encode_as_pieces(line))\n",
        "p, r, f = calc(ans, unigram_t)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIlig1_qp5Mp",
        "outputId": "c0f6bfc2-d4b0-4f35-af45-1a8a3e696a6b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0023752969121140144\n",
            "Recall: 0.009259259259259259\n",
            "F-score: 0.003780718336483932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_t_1000=[]\n",
        "for line in pre:\n",
        "  bpe_t_1000.append(tokenizer_bpe.tokenize(line))\n",
        "p, r, f = calc(ans, bpe_t_1000)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzU31Os4uOZm",
        "outputId": "f4156888-3325-4741-8f54-8ca32752d0c3"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_t_2000=[]\n",
        "for line in pre:\n",
        "  bpe_t_2000.append(tokenizer_bpe_2000.tokenize(line))\n",
        "\n",
        "p, r, f = calc(ans, bpe_t_2000)\n",
        "\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2xIBUbAu13c",
        "outputId": "d616383c-62dc-45d3-f0f2-c1d141ba865d"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_t_1000=[]\n",
        "for line in pre:\n",
        "  bert_t_1000.append(tokenizer_bert.tokenize(line))\n",
        "p, r, f = calc(ans, bert_t_1000)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORqYDpCKwPzt",
        "outputId": "17d81f10-51cc-4060-e2f3-180c4049b194"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.059233449477351915\n",
            "Recall: 0.2361111111111111\n",
            "F-score: 0.09470752089136489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_t_2000=[]\n",
        "for line in pre:\n",
        "  bert_t_2000.append(tokenizer_bert_2000.tokenize(line))\n",
        "p, r, f = calc(ans, bert_t_2000)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1KKlU-Cxv6B",
        "outputId": "1e4e7577-415f-4600-d12d-801a967ecae7"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.059233449477351915\n",
            "Recall: 0.2361111111111111\n",
            "F-score: 0.09470752089136489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ibert_t_1000=[]\n",
        "for line in pre:\n",
        "  ibert_t_1000.append(tokenizer_bert.tokenize(line))\n",
        "p, r, f = calc(ans, ibert_t_1000)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ObNxL7iz6Xn",
        "outputId": "cf570f4e-fb06-4a3b-a552-083739f05fa3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.059233449477351915\n",
            "Recall: 0.2361111111111111\n",
            "F-score: 0.09470752089136489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ibert_t_2000=[]\n",
        "for line in pre:\n",
        "  ibert_t_2000.append(tokenizer_bert_2000.tokenize(line))\n",
        "p, r, f = calc(ans, ibert_t_2000)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smk73CAe0aIJ",
        "outputId": "e6d98a81-7830-47c3-f2e2-440207153949"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.059233449477351915\n",
            "Recall: 0.2361111111111111\n",
            "F-score: 0.09470752089136489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wst=[]\n",
        "for line in pre:\n",
        "  wst.append(line.split(\" \"))\n",
        "p, r, f = calc(ans, wst)\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F-score:\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QConl3rW00Iq",
        "outputId": "58adabf5-da10-4d6c-9569-2831b4517802"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.18793503480278423\n",
            "Recall: 0.375\n",
            "F-score: 0.25038639876352403\n"
          ]
        }
      ]
    }
  ]
}